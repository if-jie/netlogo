; Family of neurons
breed [ANN:neurons ANN:neuron]
ANN:neurons-own [
  ANN:activation   ; Activation value (output) of the neuron
  ANN:grad         ; Gradient vlue of the neuron (back-propagation)
  ANN:layer        ; Layer neuron belongs to
  ANN:neuron-type  ; Neuron Type (input, output, hidden, bias). Only informative.
]

; Family of connections in the ANN
directed-link-breed [ANN:links ANN:link]
ANN:links-own [
  ANN:weight       ; Weight of the link
  ANN:dw           ; Delta of changes in the link over a Batch
]

; Global variables for the library
globals [
  ANN:Depth          ; Number of layers
  ANN:output-neurons ; Output layer neurons
  ANN:input-neurons  ; Input layer neurons
  ANN:epoch-error    ; Current epoch error during training
  ANN:learning-rate  ; Ratio of Learning rate (back-propagation)
]

;; ---------------------------- Creation of the ANN -------------------------------
;; 
;; Procedure to create the ANN:
;;   It receives a list of int values
;;        architecture = [N-IL N-HL1 ... N-HLk N-OL]
;;   where every item is the size (number of neurons) of every layer, from input to output

to ANN:create [architecture]
  set ANN:Depth length architecture
  ; Create every layer in the network (no bias)
  (foreach architecture (range ANN:Depth)
    [ [#layer-size #idx] ->
      create-ANN:neurons #layer-size  [
        set shape "circle"
        set ANN:layer #idx
        set ANN:activation random-float 0.1
        ifelse #idx = 0 
        [set ANN:neuron-type "input"]
        [ifelse #idx = ANN:Depth - 1 
          [set ANN:neuron-type "output"]
          [set ANN:neuron-type "hidden"]
        ]
       ht 
      ]
    ])
  ; Full connect every layer with the next one and add bias neuron to hidden layers
  foreach (range (ANN:Depth - 1))
  [ #lay ->
    ANN:connect (ANN:neurons-layer #lay) (ANN:neurons-layer (#lay + 1))
    ; If it is a hidden layer, we create bias and connet them to it
    if #lay > 0 [
      create-ANN:neurons 1 [ 
        set ANN:activation 1 
        set ANN:neuron-type "bias"
        set shape "circle"
        set color yellow
        ANN:connect self (ANN:neurons-layer #lay)
        set ANN:layer #lay
        ht
      ]
    ]
  ]
  ; Store the input and output layers to optimize some processes
  set ANN:output-neurons ANN:neurons-layer (ANN:Depth - 1)
  set ANN:input-neurons ANN:neurons-layer 0
end

;; Auxiliary procedure to connect two groups of neurons
to ANN:connect [#neurons1 #neurons2]
  ask #neurons1 [
    create-ANN:links-to #neurons2 [
      set ANN:weight random-float 0.2 - 0.1
      hide-link
    ]
  ]
end

;; Report the neurons of a layer (by index, 0: input, ...)
to-report ANN:neurons-layer [#n]
  report ANN:neurons with [ANN:layer = #n]
end

;; Function that the ANN computes. 
;;   Receives a list of input values [x1...xn]
;;   Reports the activations of the output layer   
to-report ANN:compute [#xs]
  ANN:set-input #xs
  ANN:Forward-Propagation
  report map [#n -> [ANN:activation] of #n] (sort ANN:output-neurons)
end

;; Pass the inputs values to the input layer
to ANN:set-input [#xs]
  (foreach (sort ANN:input-neurons) #xs
    [ [#n_i #x_i] -> ask #n_i [set ANN:activation #x_i] ])
end

;; Forward Propagation of the signal along the network
to ANN:Forward-Propagation
  foreach (range 1 ANN:Depth)
  [ #n ->
    ask (ANN:neurons-layer #n) [ set ANN:activation ANN:compute-neuron-activation ]
  ]
end

;;  Activation of one neuron
to-report ANN:compute-neuron-activation
  report sigmoid sum [[ANN:activation] of end1 * ANN:weight] of my-in-ANN:links
end

; Sigmoid Function
to-report sigmoid [x]
  report 1 / (1 + e ^ (- x))
end

to-report sigmoid' [x]
  report x * (1 - x)
end

to-report tanh [x]
  report ((e ^ x) - (e ^ (- x))) / ((e ^ x) + (e ^ (- x)))
end

to-report tanh' [x]
  report 1 - x ^ 2
end

to-report LRELU [x]
  report ifelse-value (x < 0) [0.1 * x] [x]
end
  
to-report LRELU' [x]
  report ifelse-value (x <= 0) [0.1] [1]
end

; Step Function
to-report step [x]
  ifelse x > 0.5
    [ report 1 ]
    [ report 0 ]
end

;; Training Procedure
;;    Receives the number of epochs for training, the data to be used and learning rate
to ANN:train [#num-epochs #batch-size #data #lr]
  set ANN:learning-rate #lr
  let #t 0
  set ANN:epoch-error 0

  ; Loop for num-epochs iteration
  repeat #num-epochs [
    ; For every batch of trainig data
    foreach (divide (shuffle #data) #batch-size) [
      #b -> 
      ask ANN:links [set ANN:dw 0]
      ; For every training datum in the batch
      foreach #b [ 
        #d ->
        
        ; Take the input and correct output
        let #input first #d
        let #output last #d
        
        ; Set inputs on input layer
        ANN:set-input #input
      
        ; Forward step
        ANN:Forward-Propagation
        
        ; Back Propagation from the output error
        ANN:Back-propagation #output
      ]
      ask ANN:links [set ANN:weight ANN:Weight + ANN:dw]
      
    ]
    set ANN:epoch-error ANN:epoch-error / (length #data)
    ANN:external-update (list #t ANN:epoch-error)

    set #t #t + 1
  ]
end

; Divide a list in parts of size N (the last one is the rest)
to-report divide [#L #N]
  ifelse length #L <= #N 
  [report (list #L)]
  [report fput (sublist #L 0 #N) (divide (sublist #L #N (length #L)) #N)]
end
  

; Back Propagation from the output error
to ANN:Back-propagation [#output]
  let #error-sample 0
  ; Compute error and gradient of every output neurons
  (foreach (sort ANN:output-neurons) #output [
    [#n_i #o_i] ->
    ask #n_i [ set ANN:grad (sigmoid' ANN:activation) * (#o_i - ANN:activation) ]
    set #error-sample #error-sample + ( (#o_i - [ANN:activation] of #n_i) ^ 2 )
  ])
  ; Average error of the output neurons in this epoch
  set ANN:epoch-error ANN:epoch-error + (#error-sample / count ANN:output-neurons)
  ; Compute gradient of hidden layer neurons
  foreach (reverse (range 1 (ANN:Depth - 1)))
  [
    #n -> 
    ask ANN:neurons-layer #n [
      set ANN:grad (sigmoid' ANN:activation) * sum [ANN:weight * [ANN:grad] of end2] of my-out-ANN:links
    ]
  ]
  ; Update link weights
  ask ANN:links [
    set ANN:dw ANN:dw + ANN:Learning-rate * [ANN:grad] of end2 * [ANN:activation] of end1
  ]
end

;; Procedures to load/save weights of the network
;; Weights will be stored as a big vector of values (sorted)
;; Note that the arquitecture of the network is needed to
;; recover it correctly

to ANN:load [#f]
  ct
  file-open #f
  ; Read Architecture from file
  let #arch read-from-string file-read-line
  ; Create the new Network
  ANN:create #arch
  ; Read Weights from the file
  let #W read-from-string file-read-line
  ; Load new Weights into the network
  ANN:load-Weights #W
  file-close-all
end
  
to ANN:load-weights [#W]
  ; Sort the links (Netlogo will do it lexically by end1 end2)
  ; and then charge the wieghts following this order
  (foreach #W (sort ANN:links) [
    [#wij #c] -> ask #c [set ANN:weight #wij]
  ])
end   

to-report ANN:read-weights
  report map [#c -> [ANN:weight] of #c] (sort ANN:links)
end

to ANN:save [#f]
  let #arc map [#i -> count ANN:neurons-layer #i] (range ANN:Depth)
  ; Remove bias neurons from hidden layers
  let #harc map [#x -> #x - 1] (bf bl #arc)
  set #arc (lput (last #arc) (fput (first #arc) #harc))
  ; Write Architecture and Weights into the file
  file-open #f
  file-print #arc
  file-print ANN:read-weights
  file-close-all
end

;;------------------------------------------------------------
;; Procedure to pretty display the ANN
to ANN:layout
  no-display
  let #incx (world-width - 1) / (ANN:Depth - 1)
  layout-spring ANN:neurons ANN:links .1 (.5 * #incx) 3
  ask ANN:neurons [
    ifelse ANN:neuron-type = "bias" 
    [ set xcor min-pxcor + #incx  * ANN:layer  + .5 * #incx]
    [ set xcor min-pxcor + #incx  * ANN:layer ]
  ]
  display
end

to ANN:format
  ask ANN:neurons [set shape "hidden-neuron" st set color white]
  ask ANN:neurons with [ANN:neuron-type = "bias"] [set shape "bias-node" set color white]
  ask ANN:input-neurons [set shape "input-neuron" set color green]
  ask ANN:output-neurons [set shape "output-neuron2" set color blue]
  ask ANN:links [show-link]
  repeat 100 [ANN:layout]
end

to ANN:recolor-links
  ;ask ANN:neurons [
  ;  set color item (step ANN:activation) [white yellow]
  ;]
  let #MaxP max [abs ANN:weight] of ANN:links
  ask ANN:links [
    ;set thickness 0.05 * abs ANN:weight
    ifelse ANN:weight > 0
      [ set color lput (255 * abs ANN:weight / #MaxP) [0 0 255]]
      [ set color lput (255 * abs ANN:weight / #MaxP) [255 0 0]]
  ]
end
